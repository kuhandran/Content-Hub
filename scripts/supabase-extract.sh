#!/bin/bash
# Supabase Database Extraction & Monitoring Script
# Purpose: Extract, analyze, and monitor Content Hub database statistics
# Usage: ./supabase-extract.sh

set -e

# Configuration
SUPABASE_URL="${SUPABASE_URL:-https://your-project.supabase.co}"
SUPABASE_KEY="${SUPABASE_KEY:-your-anon-key}"
OUTPUT_DIR="./db_extracts"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
LOG_FILE="${OUTPUT_DIR}/extract_${TIMESTAMP}.log"

# Colors for output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

# Create output directory
mkdir -p "$OUTPUT_DIR"

log() {
  echo -e "${BLUE}[$(date +'%Y-%m-%d %H:%M:%S')]${NC} $1" | tee -a "$LOG_FILE"
}

success() {
  echo -e "${GREEN}âœ… $1${NC}" | tee -a "$LOG_FILE"
}

warning() {
  echo -e "${YELLOW}âš ï¸  $1${NC}" | tee -a "$LOG_FILE"
}

error() {
  echo -e "${RED}âŒ $1${NC}" | tee -a "$LOG_FILE"
}

log "Starting Supabase Database Extraction"
log "Timestamp: $TIMESTAMP"
log "Output Directory: $OUTPUT_DIR"

# Function to run SQL query via Supabase REST API
query_supabase() {
  local query="$1"
  local output_file="$2"
  
  log "Executing query: $query"
  # Note: This is a template - actual implementation depends on your Supabase setup
  # For production, use psql with connection string or Supabase SQL Editor
}

# ============================================================================
# 1. EXTRACT RECORD COUNTS
# ============================================================================

log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "1. EXTRACTING RECORD COUNTS"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

cat > "${OUTPUT_DIR}/record_counts_${TIMESTAMP}.sql" << 'EOF'
-- Extract record counts from all tables
SELECT 
  'menu_config' as table_name, COUNT(*) as count FROM menu_config
UNION ALL
SELECT 'collections', COUNT(*) FROM collections
UNION ALL
SELECT 'config_files', COUNT(*) FROM config_files
UNION ALL
SELECT 'data_files', COUNT(*) FROM data_files
UNION ALL
SELECT 'static_files', COUNT(*) FROM static_files
UNION ALL
SELECT 'images', COUNT(*) FROM images
UNION ALL
SELECT 'javascript_files', COUNT(*) FROM javascript_files
UNION ALL
SELECT 'resumes', COUNT(*) FROM resumes
UNION ALL
SELECT 'sync_manifest', COUNT(*) FROM sync_manifest
ORDER BY table_name;
EOF

success "Generated: record_counts_${TIMESTAMP}.sql"

# ============================================================================
# 2. EXTRACT TABLE SIZES AND METADATA
# ============================================================================

log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "2. EXTRACTING TABLE SIZES & METADATA"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

cat > "${OUTPUT_DIR}/table_sizes_${TIMESTAMP}.sql" << 'EOF'
-- Get table sizes and metadata
SELECT 
  tablename as table_name,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
  (SELECT COUNT(*) FROM information_schema.statistics 
   WHERE table_name = tablename) as index_count,
  (SELECT COUNT(*) FROM information_schema.columns 
   WHERE table_name = tablename) as column_count
FROM pg_tables 
WHERE schemaname = 'public'
ORDER BY tablename;
EOF

success "Generated: table_sizes_${TIMESTAMP}.sql"

# ============================================================================
# 3. EXTRACT COLLECTIONS BY LANGUAGE
# ============================================================================

log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "3. EXTRACTING COLLECTIONS BY LANGUAGE"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

cat > "${OUTPUT_DIR}/collections_by_language_${TIMESTAMP}.sql" << 'EOF'
-- Collections breakdown by language
SELECT 
  language,
  type,
  COUNT(*) as count,
  MIN(created_at) as first_created,
  MAX(updated_at) as last_updated
FROM collections
GROUP BY language, type
ORDER BY language, type;
EOF

success "Generated: collections_by_language_${TIMESTAMP}.sql"

# ============================================================================
# 4. EXTRACT PUMP OPERATION HISTORY
# ============================================================================

log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "4. EXTRACTING PUMP OPERATION HISTORY"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

cat > "${OUTPUT_DIR}/pump_history_${TIMESTAMP}.sql" << 'EOF'
-- Pump operation audit trail
SELECT 
  table_name,
  status,
  message,
  files_count,
  hash_mismatches,
  missing_files,
  created_at,
  updated_at
FROM sync_manifest
ORDER BY created_at DESC
LIMIT 50;
EOF

success "Generated: pump_history_${TIMESTAMP}.sql"

# ============================================================================
# 5. EXTRACT PUMP STATISTICS
# ============================================================================

log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "5. EXTRACTING PUMP STATISTICS"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

cat > "${OUTPUT_DIR}/pump_stats_${TIMESTAMP}.sql" << 'EOF'
-- Aggregate pump statistics
SELECT 
  table_name,
  status,
  COUNT(*) as operations,
  MAX(created_at) as last_operation,
  SUM(files_count) as total_files,
  AVG(files_count) as avg_files_per_operation
FROM sync_manifest
GROUP BY table_name, status
ORDER BY last_operation DESC;
EOF

success "Generated: pump_stats_${TIMESTAMP}.sql"

# ============================================================================
# 6. EXTRACT DATABASE GROWTH TREND
# ============================================================================

log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "6. EXTRACTING DATABASE GROWTH TREND"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

cat > "${OUTPUT_DIR}/growth_trend_${TIMESTAMP}.sql" << 'EOF'
-- Database growth over time (last 30 days)
SELECT 
  DATE_TRUNC('day', created_at)::date as date,
  COUNT(DISTINCT table_name) as tables_updated,
  SUM(files_count) as files_processed,
  COUNT(*) as operations,
  MAX(status) as last_status
FROM sync_manifest
WHERE created_at >= NOW() - INTERVAL '30 days'
  AND status = 'completed'
GROUP BY DATE_TRUNC('day', created_at)
ORDER BY date DESC;
EOF

success "Generated: growth_trend_${TIMESTAMP}.sql"

# ============================================================================
# 7. EXTRACT FILE HASH ANALYSIS (for change detection)
# ============================================================================

log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "7. EXTRACTING FILE HASH ANALYSIS"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

cat > "${OUTPUT_DIR}/file_analysis_${TIMESTAMP}.sql" << 'EOF'
-- File hash distribution (to detect changes)
SELECT 
  'collections' as table_name,
  COUNT(*) as total_files,
  COUNT(DISTINCT file_hash) as unique_hashes,
  COUNT(*) - COUNT(DISTINCT file_hash) as potential_duplicates
FROM collections

UNION ALL

SELECT 
  'config_files',
  COUNT(*) as total_files,
  COUNT(DISTINCT file_hash),
  COUNT(*) - COUNT(DISTINCT file_hash)
FROM config_files

UNION ALL

SELECT 
  'data_files',
  COUNT(*),
  COUNT(DISTINCT file_hash),
  COUNT(*) - COUNT(DISTINCT file_hash)
FROM data_files

UNION ALL

SELECT 
  'static_files',
  COUNT(*),
  COUNT(DISTINCT file_hash),
  COUNT(*) - COUNT(DISTINCT file_hash)
FROM static_files

UNION ALL

SELECT 
  'images',
  COUNT(*),
  COUNT(DISTINCT file_hash),
  COUNT(*) - COUNT(DISTINCT file_hash)
FROM images

UNION ALL

SELECT 
  'javascript_files',
  COUNT(*),
  COUNT(DISTINCT file_hash),
  COUNT(*) - COUNT(DISTINCT file_hash)
FROM javascript_files

UNION ALL

SELECT 
  'resumes',
  COUNT(*),
  COUNT(DISTINCT file_hash),
  COUNT(*) - COUNT(DISTINCT file_hash)
FROM resumes;
EOF

success "Generated: file_analysis_${TIMESTAMP}.sql"

# ============================================================================
# 8. EXTRACT LATEST DATABASE STATUS
# ============================================================================

log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "8. EXTRACTING LATEST DATABASE STATUS"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

cat > "${OUTPUT_DIR}/database_status_${TIMESTAMP}.sql" << 'EOF'
-- Current database status snapshot
WITH stats AS (
  SELECT 
    'menu_config' as table_name, COUNT(*) as count FROM menu_config
  UNION ALL
  SELECT 'collections', COUNT(*) FROM collections
  UNION ALL
  SELECT 'config_files', COUNT(*) FROM config_files
  UNION ALL
  SELECT 'data_files', COUNT(*) FROM data_files
  UNION ALL
  SELECT 'static_files', COUNT(*) FROM static_files
  UNION ALL
  SELECT 'images', COUNT(*) FROM images
  UNION ALL
  SELECT 'javascript_files', COUNT(*) FROM javascript_files
  UNION ALL
  SELECT 'resumes', COUNT(*) FROM resumes
  UNION ALL
  SELECT 'sync_manifest', COUNT(*) FROM sync_manifest
)
SELECT 
  COUNT(*) as total_tables,
  SUM(count) as total_records,
  MIN(count) as min_records,
  MAX(count) as max_records,
  ROUND(AVG(count)::numeric, 0) as avg_records_per_table
FROM stats;
EOF

success "Generated: database_status_${TIMESTAMP}.sql"

# ============================================================================
# 9. GENERATE SUMMARY REPORT
# ============================================================================

log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "9. GENERATING SUMMARY REPORT"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

cat > "${OUTPUT_DIR}/README_${TIMESTAMP}.md" << 'EOF'
# Supabase Database Extraction Report

**Generated**: $(date)
**Database**: Content Hub Application
**Timestamp**: ${TIMESTAMP}

## ðŸ“‹ Files Generated

1. **record_counts_${TIMESTAMP}.sql**
   - Count of records in each table
   - Run this to get total records per table

2. **table_sizes_${TIMESTAMP}.sql**
   - Table sizes, index counts, column counts
   - Shows storage usage and schema info

3. **collections_by_language_${TIMESTAMP}.sql**
   - Records broken down by language
   - Shows creation and update timestamps

4. **pump_history_${TIMESTAMP}.sql**
   - Complete pump operation history (last 50)
   - Status, files processed, errors

5. **pump_stats_${TIMESTAMP}.sql**
   - Aggregated pump statistics
   - Success/failure rates by table

6. **growth_trend_${TIMESTAMP}.sql**
   - Database growth over last 30 days
   - Files processed per day

7. **file_analysis_${TIMESTAMP}.sql**
   - File hash distribution
   - Detects duplicate/changed files

8. **database_status_${TIMESTAMP}.sql**
   - Current database status snapshot
   - Aggregate statistics

## ðŸš€ How to Use

### Option 1: Run in Supabase SQL Editor
1. Open your Supabase project
2. Go to SQL Editor
3. Open each .sql file
4. Copy and paste the query
5. Execute and view results

### Option 2: Run via psql
```bash
psql $DATABASE_URL -f record_counts_${TIMESTAMP}.sql
psql $DATABASE_URL -f table_sizes_${TIMESTAMP}.sql
# etc...
```

### Option 3: Import to Data Manager Dashboard
The Data Manager tab automatically runs these queries every 5 seconds!

## ðŸ“Š Expected Results

### Record Counts
```
menu_config        | 12
collections        | 45+
config_files       | 8
data_files         | 120+
static_files       | 35
images             | 22
javascript_files   | 18
resumes            | 6
sync_manifest      | 1+
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€
TOTAL              | 271+
```

### Database Health
- âœ… Total Tables: 9
- âœ… Total Records: 271+
- âœ… Storage: 5-10 MB
- âœ… Last Updated: Just now

## ðŸ” What Each Query Tells You

| Query | Shows | Used For |
|-------|-------|----------|
| record_counts | Total records per table | Data volume, completeness |
| table_sizes | Storage size, indexes | Performance, optimization |
| collections_by_language | Language distribution | Multi-language support status |
| pump_history | Operation details | Debugging, audit trail |
| pump_stats | Statistics summary | Success rates, trends |
| growth_trend | Historical growth | Capacity planning |
| file_analysis | Hash distribution | Change detection |
| database_status | Aggregate view | Overall health |

## ðŸŽ¯ Monitoring Schedule

- **Every 5 seconds**: Data Manager dashboard (auto)
- **Every 15 minutes**: Manual check via Supabase
- **Daily**: Run extraction script and review
- **Weekly**: Analyze trends and growth

## ðŸ“ˆ Success Criteria

After running "Load Primary Data" pump:
- menu_config: 12 records (unchanged)
- collections: 45-55 records
- config_files: 8 records
- data_files: 120+ records
- static_files: 30-40 records
- images: 20+ records
- javascript_files: 15+ records
- resumes: 5+ records
- sync_manifest: 1+ record (pump log entry)

## âš ï¸ Troubleshooting

- **0 records in most tables**: Run pump operation first
- **Empty sync_manifest**: Pump hasn't run yet
- **Database errors**: Check error messages in pump_history
- **Slow queries**: Check table indexes exist

## ðŸ“ž Support

If issues arise:
1. Check pump_history_${TIMESTAMP}.sql for errors
2. Review database_status_${TIMESTAMP}.sql for health
3. Run collection_by_language_${TIMESTAMP}.sql for data distribution
4. Monitor pump_stats_${TIMESTAMP}.sql for patterns

Generated: $(date)
EOF

success "Generated: README_${TIMESTAMP}.md"

# ============================================================================
# COMPLETION SUMMARY
# ============================================================================

log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "EXTRACTION COMPLETE"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

echo ""
success "All SQL queries generated successfully!"
success "Output directory: $OUTPUT_DIR"
success "Files created:"
ls -lh "$OUTPUT_DIR"/*_${TIMESTAMP}.* 2>/dev/null || echo "Files ready for use"

echo ""
log "Next Steps:"
log "1. Open Supabase SQL Editor"
log "2. Copy each query from the .sql files"
log "3. Execute to get real-time database stats"
log "4. Or use Data Manager dashboard (auto-updates)"

echo ""
success "Extraction script completed at $(date)"
